# 图神经网络上的自适应迁移学习

2021，KDD（国际数据挖掘顶级会议）

## 摘要

图神经网络 (GNN) 被广泛用于图数据学习。最近的工作表明，将知识从自监督任务转移到下游任务可以进一步改善图表示。然而，自监督任务和下游任务在优化目标和训练数据方面存在固有差距。传统的预训练方法在知识转移方面可能不够有效，因为它们没有对下游任务进行任何适应。为了解决这些问题，我们在 GNN 上提出了一种新的迁移学习范式，它可以有效地利用自监督任务作为辅助任务来帮助目标任务。我们的方法会在微调阶段自适应地选择和组合不同的辅助任务与目标任务。我们设计了自适应辅助损失加权模型，通过量化辅助任务与目标任务之间的一致性来学习辅助任务的权重。此外，我们通过元学习来学习加权模型。我们的方法可以应用于各种迁移学习方法，它不仅在多任务学习中表现良好，而且在预训练和微调中也表现良好。对多个下游任务的综合实验表明，与最先进的方法相比，所提出的方法可以有效地将辅助任务与目标任务结合起来，并显着提高性能。

## 介绍

**自监督学习**：自我监督方法可以看作是一种具有监督形式的特殊形式的非监督学习方法，这里的监督是由自我监督任务而不是预设先验知识诱发的。与完全不受监督的设置相比，*自监督学习使用数据集本身的信息来构造伪标签*。

**目前GNN迁移学习的问题：**GNN 通常以端到端的方式进行训练，这通常需要大量的标记数据，但对于许多图数据集来说，特定于任务的标签可能很少。最近的一些工作研究了 GNN 预训练和使用辅助任务来解决此类问题。他们的目标是将学习到的知识从自监督任务转移到下游任务，这些方法利用丰富的未标记数据可以进一步提高图表示。然而，由于自监督任务和下游任务之间优化目标和数据分布的差异，预训练和微调参数之间存在差距。此外，传统的预训练方法不知道哪些自监督任务对下游任务更有利，并且不对其进行任何调整。这导致知识转移对于下游任务不够有效。在提高迁移学习在 GNN 上的有效性、充分利用自我监督中的信息以及对下游任务进行充分适应方面几乎没有探索。



